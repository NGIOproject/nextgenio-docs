

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Submitting Jobs &mdash; nextgenio-docs  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Scheduler" href="data_scheduler.html" />
    <link rel="prev" title="Connecting to the System" href="connect.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nextgenio-docs
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware.html">System Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="connect.html">Connecting to the System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Submitting Jobs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#slurm">Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basic-commands">Basic Commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-subsmission-scripts">Example Subsmission Scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#debugging">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#should-i-use-mpirun-or-srun">Should I Use <em>mpirun</em> or <em>srun</em>?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#slurm-on-nextgenio">Slurm on NextgenIO</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_scheduler.html">Data Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="file_systems.html">File Systems and Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilers.html">Modules and Compilers</a></li>
<li class="toctree-l1"><a class="reference internal" href="pycompss.html">PyCOMPSs</a></li>
<li class="toctree-l1"><a class="reference internal" href="perftools.html">Performance Analysis Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs and Common Issues</a></li>
</ul>
<p class="caption"><span class="caption-text">Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apps/castep.html">CASTEP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/halvade.html">Halvade</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/kmeans.html">K-Means</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/monc.html">MONC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/isf.html">RSF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/openfoam.html">OpenFOAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/ospray.html">OSPRay</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apps/tiramisu.html">Tiramisu</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nextgenio-docs</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Submitting Jobs</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/user_guide/job_scheduler.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="submitting-jobs">
<span id="sec-ref-scheduler"></span><h1>Submitting Jobs<a class="headerlink" href="#submitting-jobs" title="Permalink to this headline">¶</a></h1>
<p>The NextgenIO system makes use of the an adapted version of the <a class="reference external" href="https://slurm.schedmd.com/overview.html">Slurm
workload manager</a>. The
adaptations make it possible for the user to make use of the various
implementations of SCM (Storage Class Memory).</p>
<p>This section will first introduce the basic usage of Slurm, with the help
of some simple examples. The second part of the will expand on this, and
introduce the new usage of Slurm on the NextgenIO system.</p>
<div class="section" id="slurm">
<h2>Slurm<a class="headerlink" href="#slurm" title="Permalink to this headline">¶</a></h2>
<p>This section will provide a short overview of some of the basic commands
required to run jobs with Slurm.</p>
<p>Please note: the NextgenIO system comes equiped with OpenMPI (version
3.1.0) and Intel MPI (version 2019.3.199). The usage of Slurm should
reflect the choice of MPI flavour, as will indicated in the guides in
this section where relevant.</p>
<p>For a detailed manual on the use of Slurm, please consult the <a class="reference external" href="https://slurm.schedmd.com/documentation.html">Slurm
documentation pages</a>.</p>
<div class="section" id="basic-commands">
<h3>Basic Commands<a class="headerlink" href="#basic-commands" title="Permalink to this headline">¶</a></h3>
<p><strong>System Status</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>sinfo</td>
<td><div class="first last line-block">
<div class="line">list available partitions, nodes on these partitions and the status of</div>
<div class="line">these components. Availability is listed as either <em>up</em> or <em>down</em>.</div>
<div class="line">For a full list of node-by-node status enter: <em>sinfo –long –Node</em></div>
</div>
</td>
</tr>
<tr class="row-even"><td>squeue</td>
<td><div class="first last line-block">
<div class="line">list the jobs currently submitted to the system. Basic functionality</div>
<div class="line">returns all jobs, provides the JOBID, the job owner, the requested</div>
<div class="line">number of nodes and either the allocated nodes or the reason for being</div>
<div class="line">queued.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td>scontrol</td>
<td><div class="first last line-block">
<div class="line">combined with options <em>show node</em> it will list more detailed, node</div>
<div class="line">specific information. The nextgenio compute nodes are labelled</div>
<div class="line"><em>nexgentio-cn[xx]</em>. An example command:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">node</span> <span class="pre">nextgenio-cn01</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
<p><strong>Submitting Jobs</strong></p>
<p>Submitting a job using the scheduler comprises two steps: allocating
resources and specifying the job itself. Slurm support three main
methods for launching jobs: <a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a>,
<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a>, and <a class="reference external" href="https://slurm.schedmd.com/sbatch.html">sbatch</a>.</p>
<p>OpenMPI jobs can be run using all three method, and for Intel
MPI jobs the <code class="docutils literal notranslate"><span class="pre">srun</span></code> method is recommended. Slurm is compatible
with the mpirun method for both flavours.</p>
<p>Below is a quick overview of the command usage, with more
detailed examples listed further down.</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>srun</td>
<td><div class="first last line-block">
<div class="line">submit a job by allocating resources and specifying the executable.</div>
<div class="line">Basic example, running ‘mycode’ on 2 nodes:</div>
<div class="line-block">
<div class="line">$&gt; srun -N2 mycode</div>
</div>
</div>
</td>
</tr>
<tr class="row-even"><td>salloc</td>
<td><div class="first last line-block">
<div class="line">allocate resources. Adding ‘sh’ at the end of the command launches an</div>
<div class="line">interactive shell, whence to start the job. Basic example, requesting 4</div>
<div class="line">nodes:</div>
<div class="line-block">
<div class="line">$&gt; salloc -N4 sh</div>
<div class="line">&gt; srun mycode (OR: mpirun mycode)</div>
</div>
</div>
</td>
</tr>
<tr class="row-odd"><td>sbatch</td>
<td><div class="first last line-block">
<div class="line">submit a batch script to the scheduler. Resource allocation can be</div>
<div class="line">specified in the command line. The batch file should contain a statement</div>
<div class="line">the job. A script can contain multiple srun/mpirun commands:</div>
<div class="line-block">
<div class="line">$&gt; sbatch -N4 myscript.sh</div>
</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>The commands and examples listed in this section are intended only
to show the basic functionality of the scheduler. All commands have
many available options and switches, and are therefore highly adaptable
to user requirements. The Slurm documentation provides full details
and examples for the interested reader.</p>
<p>To cancel a submitted job, use <a class="reference external" href="https://slurm.schedmd.com/scancel.html">scancel</a> followed by the JobID. To cancel all of a user’s jobs
enter <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-u</span> <span class="pre">[username]</span></code>.</p>
</div>
<div class="section" id="example-subsmission-scripts">
<h3>Example Subsmission Scripts<a class="headerlink" href="#example-subsmission-scripts" title="Permalink to this headline">¶</a></h3>
<p>The following examples (based on the helpful tutorial found <a class="reference external" href="https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html">here</a>) are batch scripts for different examples
of parallel runs.</p>
<p>In all cases the scripts illustrate the basic usage of the scheduler:
allocate resources, specify the job to be run, and then run it. We will
also specify the job output location. The default location for both
standard out and standard error is the file “slurm-j[JOB_ID]”. In the
case of using batch scripts, the commands to be passed to the scheduler
are preceded by the the <cite>#SBATCH</cite> comment.</p>
<p>Upon submission of the script the job will be added to the queue, until
the requested resources become available and the job is launched.</p>
<p><strong>1) Submitting a single job many times (without shared memory)</strong></p>
<p>This type of parallellism is usually associated with embarrassingly
parallel problems.</p>
<p>The first lines of the bash script (“myscript.sh”) allocates
resources. Note that the default allocation of CPUs is one per
taks. Default units for the memory per CPU are MB. The ‘array’
option creates multiple instances of the same task (in this
example 10 instances). The task IDs are stored in the variable
SLURM_ARRAY_TASK_ID. We will also set the maximum run time to
one hour per cpu.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH --time=60:00</span>
<span class="c1">#SBATCH --mem-per-cpu=150</span>
<span class="c1">#SBATCH --array=1-10</span>
</pre></div>
</div>
<p>The next lines specify the job name, the  output filename,
and the directory the input and output are stored. The most
reliable method is to always specify the full path (see also
<a class="reference internal" href="faq.html#ref-qnojobfile"><span class="std std-ref">Slurmstepd: error: execve(): [exec-name] : No such file or directory</span></a>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --job-name=par_job</span>
<span class="c1">#SBATCH --output=op_file.txt</span>
<span class="c1">#SBATCH -D /path/to/directory</span>
</pre></div>
</div>
<p>And finally we tell the scheduler to run the job, passing
the task IDs to the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>srun myexec $SLURM_ARRAY_TASK_ID
</pre></div>
</div>
<p>We then submit the script with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p><strong>2) Submitting a single job with shared memory using OpenMP</strong></p>
<p>In case the script to be run uses multithreading, multiple
CPUs can be asigned to the same task. For the job to run
properly, we do need to explicitly tell Slurm to set the
OPM_NUM_THREADS environment variable.</p>
<p>The rest of the script looks very similar to the previous
case:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=60:00
#SBATCH --mem-per-cpu=150
#
#SBATCH --job-name=openmp_job
#SBATCH --output=op_file.txt

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
srun myexec
</pre></div>
</div>
<p>The script is submitted by entering:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p><strong>3) Submitting an MPI job</strong></p>
<p>When submitting an MPI job the script only needs to specify
the number of tasks and, if desired, the amount of memory per core.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --ntasks=10</span>
<span class="c1">#SBATCH --time=60:00</span>
<span class="c1">#SBATCH --mem-per-cpu=150</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --job-name=mpi_job</span>
<span class="c1">#SBATCH --output=op_file.txt</span>

<span class="n">srun</span> <span class="n">myexec</span>
</pre></div>
</div>
<p>The script is submitted by entering:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p><strong>4) Submitting a hybrid MPI/OpenMP job</strong></p>
<p>For a job that combines MPI and multithreading the most important
part is to allocate the correct number of cores, to be passed as the
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREAD</span></code> variable.</p>
<p>The following script requests four nodes (total number of cpus=4*48=
192). Two MPI processes are requested per node, as each node has
two sockets this should allocate one process per socket. The script
requests all physical cores on the node, with a 1:1 ratio of threads
to physical cores (i.e. not making use of hyperthreading).</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span>!#/bin/bash
#SBATCH --nodes=4
#SBATCH --ntasks=8
#SBATCH --ntasks-per-node=2      ##This should be scheduled automatically
#SBATCH --cpus-per-task=24       ##This should be scheduled automatically

#SBATCH --job-name=mpi-omp-job

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

srun myexec
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p><strong>Hyperthreading: difference between srun and mpirun</strong></p>
<p>For most job submissions it makes no difference whether mpirun
or srun is used to execute the job. However, there is a difference
in how the two count the number of available cores <em>when using
hyperthreading</em>.</p>
<p>When requesting a number of cores (per MPI process), using
<code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code>, mpirun will allocate this as the number of
<strong>logical</strong> cores, whereas srun will use this number to allocate
<strong>physical</strong> cores, unless the option <em>–overcommit</em> is passed to it.</p>
<p>Passing the option to srun in a batch script can be done by adding the
line <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--overcommit</span></code>.</p>
<p class="last">If one attempts to request all available logical cores on a node
using srun, this may result in the error <a class="reference internal" href="faq.html#ref-qconfig"><span class="std std-ref">Batch job submission failed: Requested node configuration is not available:</span></a>.</p>
</div>
<p><strong>5) Pinning processes and binding threads</strong></p>
<p>If components of a job need to be pinned to specific nodes or cores, this
can be specified in the batch script as well. See also the <a class="reference external" href="https://slurm.schedmd.com/mc_support.html">Slurm documentation
on Multi-core support</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>If you would like to seem more information on CPU affinity of MPI processes
add the following line to the batch script:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">I_MPI_DEBUG</span><span class="o">=</span><span class="mi">5</span>
</pre></div>
</div>
<p>For more information on thread affinity, inlclude the <em>verbose</em> option in the
<em>KMP_AFFINITY</em> variable:</p>
<div class="code bash last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">verbose</span>
</pre></div>
</div>
</div>
<p>The number identification of the cores is on a per node basis. The physical cores
are therefore labelled 0–47. When using hyperthreading (which is enabled by
default on the NextgenIO system) the logical cores are labelled 48-95, where core
48 corresponds to physical core 0, 49 to 1, and so on.</p>
<p><em>Pinning MPI processes</em></p>
<p>When using mpirun, the MPI processes can be pinned to a specfic core by setting
the environment variable <em>I_MPI_PIN_PROCESSOR_LIST</em>. To pin four MPI processes
to (e.g.) the first four CPUs among the allocated CPUs, add the following line to the
batch script:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">I_MPI_PIN_PROCESSOR_LIST</span><span class="o">=</span><span class="mi">0</span><span class="o">-</span><span class="mi">3</span>
</pre></div>
</div>
<p>When using srun, the pinning of MPI processes can be done by setting the
option <code class="docutils literal notranslate"><span class="pre">--cpu-bind=map_cpu:[cpu_id(s)]</span></code>, where <em>[cpu_id(s)]</em> is a comma
separated list of cores to which the processes should be bound. Note that it
is not possible to specify a range of CPUs in the same manner as when using
mpirun: it will be necessary to write out the list of cpu_ids in full.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When pinning MPI processes to specific cores, any threads the process will
create will run on <em>the same physical core</em> as the MPI process. When running
mutiple threads per MPI process, the more reliable way of fixing core affinity
is to allow the job scheduler to allocate the CPUs for the MPI processes
(possibly modified with the <code class="docutils literal notranslate"><span class="pre">--distribution</span></code> option, see below), and then to
bind the threads within each proces.</p>
</div>
<p><em>Binding threads</em></p>
<p>To bind threads to specfic cores the batch script needs to set the evironment
variables <a class="reference external" href="https://gcc.gnu.org/onlinedocs/libgomp/OMP_005fPROC_005fBIND.html">*OMP_PROC_BIND*</a>
and <a class="reference external" href="https://gcc.gnu.org/onlinedocs/libgomp/OMP_005fPLACES.html#OMP_005fPLACES">*OMP_NUM_PLACES*</a>.
The first of the variables simply needs to be set to <em>TRUE</em>, for the second
there are multiple options available (see the documentation for a full list).</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">OMP_NUM_PLACES=cores</span></code> pins threads to the physical cores they are
assigned to, but allows them to migrate between the two logical cores on each
physical core. Setting <code class="docutils literal notranslate"><span class="pre">OMP_NUM_PLACES=thread</span></code> pins threads to the logical
core to which they are set.</p>
<p>The following example submission scrip populates every logical core on two
nodes (total number of cores = 2*48*2 = 192) with a single thread, and pins
the thread to that logical core. The threads are spread over four MPI processes
(set using <code class="docutils literal notranslate"><span class="pre">--ntasks=4</span></code>), therefore the variable <em>OMP_NUM_THREADS</em> needs to be
set to to 48 (= 192 / 4).</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH --overcommit             ##Neccessary because we use srun with hyperthreading</span>

<span class="c1">#SBATCH --job-name=mpi_omp_run</span>
<span class="c1">#SBATCH --output=opmix.txt</span>

<span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">48</span>
<span class="n">export</span> <span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">TRUE</span>
<span class="n">export</span> <span class="n">OMP_PLACES</span><span class="o">=</span><span class="n">threads</span>
<span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">compact</span>

<span class="n">srun</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">myexec</span>
</pre></div>
</div>
<p>One level above the manual pinning of threads is the setting of the core affinity
for the multithreading. This can be done by setting the <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> environment
variable. Note that including the option <em>verbose</em> for this variable prints additiional
core affinity information to output.</p>
<p>The most important option for <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> are <em>compact</em> and <em>scatter</em>. <em>compact</em>
places subsequent threads on CPUs as closely together as possible. <em>scatter</em> distributes
threads by placing them on CPUs that are spaced apart as much as possible. To use these
options via a batch script and show the results in output, add (e.g.) the following line:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span><span class="n">compact</span>
</pre></div>
</div>
<p>The level on which these options take effect can be specified with the <em>granularity</em>
option. This can be set to <em>fine</em> for distribution on the level of physical CPUs, and
to <em>thread</em> for hyperthreading.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> variable also allows for the explicit binding of threads to cores,
using the <em>explicit</em> option, followed by the <em>proclist</em> options specifying the cpu_id(s)
(note the double quotes arround the options in this case):</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="s2">&quot;explicit,proclist=[cpu_id1,cpu_id2,...,cpu_idN]&quot;</span>
</pre></div>
</div>
<p>Unfortunately, pinning of threads within MPI processes does not seem to be possible using
this option. This option would therefore only be of use for job consisting of a single
process (with mutiple threads).</p>
<p>Some further examples of usage of <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> are provided on the website for
<a class="reference external" href="https://www.nas.nasa.gov/hecc/support/kb/using-intel-openmp-thread-affinity-for-pinning_285.html">NASA’s HECC</a>.
This website also includes a visual example of the effects of <em>compact</em> and <em>scatter</em>
on thread distribution.</p>
<p><em>Other task distribution options</em></p>
<p>In the example above the option <code class="docutils literal notranslate"><span class="pre">--cpus-per-task</span></code> is not set, as the job scheduler
should allocate the optimal number of cores automatically. Similarly, the option
<code class="docutils literal notranslate"><span class="pre">--ntasks-per-socket</span></code> is only of use if an unusual configuration of MPI processes
is desired. The standard distribution enforced by the job scheduler is to spread
processes evenly accross sockets: if the number of processes matches the number of
sockets, one process will be places in each socket.</p>
<p>The allocation of MPI processes and threads can further be controlled with the
<code class="docutils literal notranslate"><span class="pre">--distribution</span></code> option. This is a complicated option, with many settings. The
basic example below (which would be included in the batch script) tells the scheduler
to allocate in a cyclic manner, i.e. per node or per socket, and the threads in a
block manner, i.e. all together:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --distribution cyclic:block</span>
</pre></div>
</div>
<p>The first part of the option’s settings set the distribution of the tasks, the
second sets the distribution of the threads. The <code class="docutils literal notranslate"><span class="pre">cyclic:block</span></code> matches the
default allocation style of the job scheduler. As with the other pinning and
allocation settings described in this section, these options should only be invoked
by users wishing to create a specific configuration.</p>
</div>
<div class="section" id="debugging">
<h3>Debugging<a class="headerlink" href="#debugging" title="Permalink to this headline">¶</a></h3>
<p>If code compiles but the executable still requires debugging, <em>impi</em> allows
for additional debuggin information to be set using the <a class="reference external" href="https://software.intel.com/en-us/mpi-developer-reference-linux-other-environment-variables">I_MPI_DEBUG</a>
environment variable. The argument for for the variable is the level of
debugging. Setting the variable to zero disables the printing of debugging
information, and positive integers enable printing, with increasing level
of detail.</p>
<p>This option can be included in a batch script by adding the following line (e.g):</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">I_MPI_DEBUG</span><span class="o">=</span><span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="section" id="should-i-use-mpirun-or-srun">
<span id="srun-or-mpirun"></span><h3>Should I Use <em>mpirun</em> or <em>srun</em>?<a class="headerlink" href="#should-i-use-mpirun-or-srun" title="Permalink to this headline">¶</a></h3>
<p>Both <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> and <code class="docutils literal notranslate"><span class="pre">srun</span></code> can be used to tell the job scheduler to run
an executable. Although the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command makes use of <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> to
execute a job, there are subtle differences in the way settings, such as
environment variables set in a batch script or shell, are passed to each
command. Below is a list of differences to be aware off when switching
between the two commands:</p>
<ul class="simple">
<li>mpirun has its own set of commands to pass task distribution commands
to the scheduler: <code class="docutils literal notranslate"><span class="pre">-n</span></code> sets the number of tasks (MPI processes) to be
created and <code class="docutils literal notranslate"><span class="pre">-ppn</span></code> sets the number of processes to be placed per node.
The <code class="docutils literal notranslate"><span class="pre">-hosts</span></code> option can be used to specify the specific nodes to be used.
Note, however, that all these setting are <em>overwritten</em> by the job scheduler
if task allocation instructions are passed to the scheduler directly (e.g. by
setting <code class="docutils literal notranslate"><span class="pre">-ntasks</span></code> in the submission script or shell).</li>
<li>…</li>
</ul>
</div>
</div>
<div class="section" id="slurm-on-nextgenio">
<h2>Slurm on NextgenIO<a class="headerlink" href="#slurm-on-nextgenio" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Some</span> <span class="n">examples</span> <span class="n">are</span> <span class="n">probably</span> <span class="n">the</span> <span class="n">quickest</span> <span class="n">to</span> <span class="n">show</span> <span class="n">the</span> <span class="n">way</span> <span class="n">here</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="data_scheduler.html" class="btn btn-neutral float-right" title="Data Scheduler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="connect.html" class="btn btn-neutral float-left" title="Connecting to the System" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, nextgenio_project

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>